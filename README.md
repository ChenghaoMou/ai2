# David's Code Base For AI2 Commonsense Leaderboard

## Install Dependencies and Set Up

Create and run a virtual environment with Python 3.7 using anaconda. Make sure to use conda version `>=4.8.2`.

```bash
conda create --name ai2_stable python=3.7
conda activate ai2_stable
pip install -r python_environment/requirements_pip.txt
```

Then download pretrained weights on multiple pretrained models that are used in the repository

```bash
python download_pretrained_weights.py
```

This repo uses the Hydra python module to handle configuration and result storage. The config files are in the yaml 
format, and the results are stored in multirun or outputs folder based on the time when the script is executed. 
For more information in how to use Hydra please reference their website: https://hydra.cc/

## General File Structure Information
    .
    ├── config                      # Configuration Files
    │   ├── task                    # Configuration for each task, with train/dev file locations
    │   ├── model                   # Configuration files for each model, with max epoch, learning rate, etc.
    │   └── checkpoint_list         # Lists of location where checkpoint files are located
    ├── model_cache                 # Hugging face Model Cache for Transformers 
    ├── multirun or outputs         # Hydra python package output folders - this is where script output should be
    ├── python_environment          # Python environment specification files
    ├── slurm                       # Slurm job submission scripts for HPC
    ├── task_data                   # Data folder for AI2 challenges
    ├── utilities                   # Helper classes for core python scripts
    ├── core python scripts         # Model, Train, Eval, Embed, Closest
    └── README.md

## train.py

This script is the script that generates a model checkpoint by fine tuning it on a specific dataset. It can also handle
loading in an existing model and further fine tune it with more data. The script largely uses the framework provided
by pytorch-lightning: https://pytorch-lightning.readthedocs.io/en/latest/

## eval.py

This script is used to evaluate a checkpoint file trained by the train script. The script uses the trained model and
evaluate it on the dev stories. It writes out the predictions, as well as the accuracy of the model and the confidence
interval of them.

## embed.py

This script uses the checkpoint files provided (or an out of box model if no checkpoints is provided) to parse the 
train and dev entries for a given task. The script output a pickled dictionary consist of the metadata for the embeddings
and embedding tuples in the form of (Name of Checkpoint File, Train Embedding, Dev Embedding). Currently the script 
supports these feature types when parsing the embeddings:
- AVG_MEAN: Average embedding of all tokens, and average of all possible answers

These features are yet to be developed:
- AVG_CORRECT: Average embedding of all tokens, and choose the one parsed with correct answer
- AVG_NULL: Average embedding of all tokens, without the options embedded in it
- CLS_MEAN, CLS_CORRECT, CLS_NULL

## closest.py

This file takes the output generated by `embed.py` and output a tsv file that lists the nearest sentences to each dev
sentence of interest using the specified distance calculation function. It can also find the farthest sentences to 
each dev sentence. You can also specify a subset of train or dev that you are interested in, or get statistics on if 
the closest samples is in a designated influential range using these two formats: (x, m-n). 
Currently the script supports the following distance functions:
- cosine
- manhattan
- euclidean
- l-0_1