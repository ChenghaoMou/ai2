## Learning rates: [2e-5, 5e-6, 2e-6]
## Batch size: [32/16, 16/8, 4/4]
## Bert base: 110 M
## Bert large: 340 M <-- large
## openai gpt: 110 M
## GPT2: 117 M  <-- weird large
## XLM: ~ 295 M <-- super large
## XLnet: 110 M
## XLNet large: 340 M <-- super large
## roberta: 125 M
## roberta large: 355 M <-- large

alphanli:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 5e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
hellaswag:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 16
    max_seq_len: 160
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
physicaliqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 5e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
socialiqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 5e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
vcrqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 5e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
vcrqr:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 5e-6
      batch_size: 8
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 5e-6
      batch_size: 4
      warmup_steps: 120
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
