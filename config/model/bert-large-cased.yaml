model: bert-large-cased
accumulate_grad_batches: 1
adam_epsilon: 1e-8
batch_size: 16
learning_rate: 5e-6
max_length: 128
precision: full # Half precision only works best with volta architectures such as V100
warmup_steps: 0
