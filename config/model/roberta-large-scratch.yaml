model: roberta-large
accumulate_grad_batches: 16
adam_epsilon: 1e-6
batch_size: 4
learning_rate: 1e-4
max_length: 128
precision: full # Half precision only works best with volta architectures such as V100
warmup_steps: 10000
