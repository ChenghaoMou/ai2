_includes:
  - "root.params"
  - "model/roberta-large.params"

parameter_options:
  task:
    - 'alphanli'
#    - 'hellaswag'
    - 'physicaliqa'
    - 'socialiqa'
  train_data_slice:
    - 50
    - 100

train:
  architecture: 'standard'
  random_seed: 42

training_overrides:
#  hellaswag:
#    parameter_options:
#      task: hellaswag
#    batch_size: 2
  alphanli:
    parameter_options:
      task: alphanli
    partition: mics
    job_time_in_minutes: 1200  # 20 hours

# Based on https://github.com/isi-vista/gaia-event-extraction/blob/d1235671952dff13b7851a96088ef2ecef4996c9/sample_params/neural_trigger_models/pegasus/experiments/tbnamm.baseline.ace.debug.params
workflow_name: george
experiment_root: '%experiments_root%/compare_models'
workflow_directory: '%pegasus_home%/compare_models'
backend: slurm
site: 'saga'
parallelism: 10
namespace: saga
partition: ephemeral

# Needed for training/inference jobs.
# Oddly, it doesn't look like it's possible to specify these at the job level,
# so I instead specify these spack requirements at the workflow level.
spack_packages:
  - "cuda@10.1.243"
  - "cudnn@7.6.5.32-9.0-linux-x64"

max_jobs_on_mics: 4

num_cpus: 4
num_gpus: 1
memory: '16g'
# sbatch --ntasks=1  # already covered, resource_request.py line 133
job_time_in_minutes: 720  # 12 horus
