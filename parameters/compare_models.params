_includes:
  - "root.params"
  - "model/roberta-large.params"

slice_options:
  - seed: 0
    percent: 70
  - seed: 42
    percent: 70
  - seed: 0
    percent: 80
  - seed: 42
    percent: 80
  - seed: 0
    percent: 90
  - seed: 42
    percent: 90

parameter_options:
  task:
    - 'physicaliqa'
    - 'socialiqa'
  random_seed:
    - 0
    - 42
    - 431
    - 774

train:
  architecture: 'standard'
  # Training should use all (100%) of the 80% slice of the data we provided
  train_data_slice: 100

# Based on https://github.com/isi-vista/gaia-event-extraction/blob/d1235671952dff13b7851a96088ef2ecef4996c9/sample_params/neural_trigger_models/pegasus/experiments/tbnamm.baseline.ace.debug.params
workflow_name: george
experiment_root: '%experiments_root%/compare_models'
workflow_directory: '%pegasus_home%/compare_models'
backend: slurm
site: 'saga'
parallelism: 10
namespace: saga
partition: scavenge

# Needed for training/inference jobs.
# Oddly, it doesn't look like it's possible to specify these at the job level,
# so I instead specify these spack requirements at the workflow level.
spack_packages:
  - "cuda@10.1.243"
#  - "cudnn@7.6.5.32-9.0-linux-x64"
  - "cudnn@7.6.5.32-9.0"

max_jobs_on_mics: 4

num_cpus: 4
num_gpus: 1
memory: '16g'
# sbatch --ntasks=1  # already covered, resource_request.py line 133
job_time_in_minutes: 720  # 12 hours
