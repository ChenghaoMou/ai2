alphanli:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
hellaswag:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 8
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 8
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 16
    max_seq_len: 160
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
physicaliqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
socialiqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
vcrqa:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
vcrqr:
  bert:
    bert-large-cased:
      lr: 5e-6
      batch_size: 16
  gpt2:
    gpt2:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      accumulate_grad_batches: 2
  gpt:
    openai-gpt:
      lr: 2e-6
      batch_size: 4
      accumulate_grad_batches: 2
  xlnet:
    xlnet-large-cased:
      lr: 5e-6
      batch_size: 16
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 16
  xlm:
    xlm-mlm-en-2048:
      lr: 2e-6
      batch_size: 4
      max_nb_epochs: 4
      warmup_steps: 120
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.1
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
