bert:
  bert-large-cased:
    lr: 1e-5
    batch_size: 8
gpt2:
  gpt2:
    lr: 2e-5
    batch_size: 16
    max_epochs: 4
xlnet:
  xlnet-large-cased:
    lr: 1e-5
    batch_size: 8
roberta:
  roberta-large:
    lr: 1e-5
    batch_size: 8
xlm:
  xlm-mlm-en-2048:
    lr: 1e-5
    batch_size: 4
    max_epochs: 4
default:
  lr: 2e-5
  dropout: 0.1
  batch_size: 32
  max_seq_len: 128
  max_epochs: 3
  initializer_range: 0.02
