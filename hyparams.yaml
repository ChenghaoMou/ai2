bert:
  bert-large-cased:
    batch_size: 8
    accumulate_grad_batches: 4
gpt2:
  gpt2:
    batch_size: 16
    accumulate_grad_batches: 2
    max_epochs: 4
  openai-gpt:
    lr: 5e-5
    batch_size: 8
xlnet:
  xlnet-large-cased:
    batch_size: 8
    accumulate_grad_batches: 4
roberta:
  roberta-large:
    batch_size: 8
    accumulate_grad_batches: 4
xlm:
  xlm-mlm-en-2048:
    batch_size: 4
    max_epochs: 4
    warmup_steps: 120
    accumulate_grad_batches: 8
default:
  seed: 42
  lr: 5e-6
  dropout: 0.1
  batch_size: 32
  max_seq_len: 160
  max_epochs: 3
  initializer_range: 0.02
  weight_decay: 0.0
  warmup_steps: 0
  adam_epsilon: 1e-8
  accumulate_grad_batches: 1
